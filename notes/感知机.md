# 感知机

## 模型介绍

二分类线性分类模型

### 基本概念

- 输入空间：$\mathcal{X} \subseteq R^n$;输入：$x = (x^{(1)},x^{(2)},...,x^{(n)})^T \in \mathcal{X}$
- 输出空间：$\mathcal{Y} = \{+1,-1\}$;输出：$y \in \mathcal{Y}$
- 感知机

$$
\mathcal{f}(x) = sign(w \cdot x + b)=
\begin{Bmatrix}
+1, w \cdot x + b \geq 0 \\
-1, w \cdot x + b \leq 0
\end{Bmatrix}
$$

其中$w = (w^{(1)},w^{(2)},...,w^{(n)})^T \in R^n$称为权值（weight），b成为偏置（bias），$w \cdot x$称为内积

- 假设空间：$\mathcal{F} = \{f|f(x) = w \cdot x + b\}$

### 几何含义

线性方程：$w \cdot x+b =0$

- 特征空间$R^n$ 中的一个超平面$S$,法向量：$w$，截距：$b$

![感知机的几何图示](..\picture\Perceptron.png)

### 学习策略

- 条件：

**数据集的线性可分性：**存在某个超平面$S$能够将给定数据集$T$的正负实例点完全正确划分到超平面的两侧，则T为线性可分数据集,否则为线性不可分

- 损失函数：

$L(w,b) = - \sum _{x_i \in M} y_i(w \cdot x_i +b)$

## 准备知识：梯度下降法

### 概念

- 梯度：某一函数在该点处最大的方向导数，沿着该方向可取的最大的变化率；$\nabla = \frac{\partial f(\theta)}{\partial \theta}$

- 若$f(\theta)$是凸函数，可通过梯度下降法优化

$$
\theta^{(k+1)} = \theta^{(k)} - \eta \nabla f(\theta^{(k)})
$$

### 原理

输入：目标函数$f(\theta)$，步长$\eta$，计算精度$\epsilon$

输出：$f(\theta)$的极小值点$\theta^*$

1. 选取初始值$\theta^{(0)} \in R^n $，置$k = 0$
2. 计算$f(\theta^{(k)})$
3. 计算梯度$\nabla f(\theta^{(k)})$
4. 置 \(\theta^{(k+1)} = \theta^{(k)} - \eta \nabla f(\theta^{(k)})\)，计算 \(f(\theta^{(k+1)})\)。当 \(\| f(\theta^{(k+1)}) - f(\theta^{(k)}) \| < \epsilon\) 或者 \(\| \theta^{(k+1)} - \theta^{(k)} \| < \epsilon\) 时，停止迭代，令 \(\theta^* = \theta^{(k+1)}\)。
5. 否则，置k = k+1，转（3）

## 学习算法

学习问题
$$
\underset{w,b}{\arg\min} L(w,b) = \underset{w,b}{\arg\min} \left[ -\sum_{x_i \in M} y_i (w \cdot x_i + b) \right]
$$

### 原始形式

#### 核心思想

直接对模型参数 w 和 b 进行迭代更新，通过逐次修正误分类样本对应的参数，使模型逐渐逼近能正确划分两类样本的超平面，核心是通过错误反馈实时调整决策边界。

#### 前提假设

模型初始参数为 \(w_0 = \mathbf{0}\)（零向量），\(b_0 = 0\)，即初始超平面为简单的原点平面。对于每个误分类样本 \((x_i, y_i)\)，根据分类错误的方向（正类被误分为负类或反之），直接调整参数 w 和 b，使超平面向正确分类该样本的方向移动。

#### 最终参数形式

经过多次迭代修正后，学习到的参数为满足以下条件的 w 和 b：

- 对所有正类样本 \((x_i, y_i=+1)\)，满足 \(w \cdot x_i + b > 0\)
- 对所有负类样本 \((x_i, y_i=-1)\)，满足 \(w \cdot x_i + b < 0\)

其中，w 为超平面的法向量（决定超平面方向），b 为超平面的截距（决定超平面位置），二者共同构成感知机模型 \(f(x) = \text{sign}(w \cdot x + b)\)。

#### 具体步骤：

- 输入：

训练集 \(T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\)，其中 \(x_i \in \mathcal{X} \subseteq \mathbb{R}^n\)，\(y \in \mathcal{Y} = \{+1, -1\}\)；步长 \(\eta(0 < \eta \leq 1)\)。

- 输出：

\(w, b\)；感知机模型 \(f(x) = \text{sign}(w \cdot x + b)\)。

- 算法步骤：

1. 选取初始值 \(w_0, b_0\)；
2. 于训练集中随机选取数据 \((x_i, y_i)\)；
3. 若 \(y_i(w \cdot x_i + b) \leq 0\)，则更新参数： \(w \leftarrow w + \eta y_i x_i\)； \(b \leftarrow b + \eta y_i\)；
4. 转步骤 2，直到训练集中没有误分类点。



### 对偶形式

#### 核心思想

从原始形式的参数迭代更新出发，通过累计误分类点的更新次数，将参数 w 和 b 表示为基于训练样本的线性组合形式，简化计算并体现样本对模型的影响权重。

#### 前提假设

初始值 \(w_0 = \mathbf{0}\)，\(b_0 = 0\)。对误分类点 \((x_i, y_i)\)，按照原始形式的更新公式（\(w \leftarrow w + \eta y_i x_i\)；\(b \leftarrow b + \eta y_i\)）进行参数修改，若某误分类点 \((x_i, y_i)\) 被修改了 \(n_i\) 次，令 \(\alpha_i = n_i \eta\)（\(\alpha_i \geq 0\)，反映该样本对参数更新的 “贡献度”）。

#### 最终参数形式

经过多次迭代更新后，学习到的参数为：

- \(w = \sum_{i=1}^{N} \alpha_i y_i x_i\)
- \(b = \sum_{i=1}^{N} \alpha_i y_i\)

其中，N 为训练样本总数，\(\alpha_i\) 是与样本 \((x_i, y_i)\) 相关的系数，体现该样本在参数更新过程中被利用的次数与步长的综合影响。

- 输入：

训练集 \(T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\)，其中 \(x_i \in \mathcal{X} \subseteq \mathbb{R}^n\)，\(y \in \mathcal{Y} = \{+1, -1\}\)；步长 \(\eta(0 < \eta \leq 1)\)。

- 输出：

\(\alpha, b\)；感知机模型 \(f(x) = \text{sign}\left( \sum_{j=1}^{N} \alpha_j y_j x_j \cdot x + b \right)\)，其中 \(\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)^T\)。

- 算法步骤：

1. 选取初始值 \(\alpha^{<0>} = (0, 0, \cdots, 0)^T\)，\(b^{<0>} = 0\)；
2. 于训练集中随机选取数据 \((x_i, y_i)\)；
3. 若 \(y_i\left( \sum_{j=1}^{N} \alpha_j y_j x_j \cdot x_i + b \right) \leq 0\)，则更新参数： \(\alpha_i \leftarrow \alpha_i + \eta\)； \(b \leftarrow b + \eta y_i\)；
4. 转步骤 2，直到训练集中没有误分类点。



迭代条件：

若 \(y_i\left( \sum_{j=1}^{N} \alpha_j y_j x_j \cdot x_i + b \right) \leq 0\)，则更新参数： \(\alpha_i \leftarrow \alpha_i + \eta\)； \(b \leftarrow b + \eta y_i\)。



进一步展开迭代条件的等式推导： \(\begin{align*} y_i\left( \sum_{j=1}^{N} \alpha_j y_j x_j \cdot x_i + b \right) &= y_i\left[ (\alpha_1 y_1 x_1 + \alpha_2 y_2 x_2 + \cdots + \alpha_N y_N x_N) \cdot x_i + b \right] \\ &= y_i\left( \alpha_1 y_1 x_1 \cdot x_i + \alpha_2 y_2 x_2 \cdot x_i + \cdots + \alpha_N y_N x_N \cdot x_i + b \right) \\ &\leq 0 \end{align*}\)

Gram 矩阵：

Gram 矩阵 G 用于存储训练样本间的内积，定义为： \(G = [x_i \cdot x_j]_{N \times N} = \begin{bmatrix} x_1 \cdot x_1 & x_1 \cdot x_2 & \cdots & x_1 \cdot x_N \\ x_2 \cdot x_1 & x_2 \cdot x_2 & \cdots & x_2 \cdot x_N \\ \vdots & \vdots & & \vdots \\ x_N \cdot x_1 & x_N \cdot x_2 & \cdots & x_N \cdot x_N \end{bmatrix}\) 通过预先计算 Gram 矩阵，在感知机对偶形式的迭代过程中，可直接调用内积结果，提升计算效率。



### 收敛性

#### 符号定义

记 \(\hat{w} = (w^T, b)^T\)，\(\hat{x} = (x^T, 1)^T\)，则分离超平面可表示为 \(\hat{w} \cdot \hat{x} = 0\)。

#### 定理内容

> 若训练集线性可分，Novikoff 定理表明：存在能完美分类且使样本与超平面有正间隔下界的最优超平面，且感知机算法在该训练集上的误分类次数有以上界 \(\left( \frac{R}{\gamma} \right)^2\) 的有限值，即算法会收敛。

若训练集 \(T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\) 线性可分，其中 \(x_i \in \mathcal{X} \subseteq \mathbb{R}^n\)，\(y \in \mathcal{Y} = \{+1, -1\}\)，则：

1. 存在满足条件 \(\|\hat{w}_{\text{opt}}\| = 1\) 的超平面 \(\hat{w}_{\text{opt}} \cdot \hat{x} = 0\) 可将 T 完全正确分开；且存在 \(\gamma > 0\)，对所有 \(i = 1, 2, \cdots, N\)，有 \(y_i(\hat{w}_{\text{opt}} \cdot \hat{x}_i) \geq \gamma\)。
2. 令 \(R = \max_{1 \leq i \leq N} \|\hat{x}_i\|\)，则感知机算法在 T 上的误分类次数 k 满足不等式 \(k \leq \left( \frac{R}{\gamma} \right)^2\)。
